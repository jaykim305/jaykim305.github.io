<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="6LTUbauzJC3AXKHMlNn5HmloDUheEVT9FVKFKveJfWo"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jaehong Kim</title> <meta name="author" content="Jaehong Kim"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaykim305.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "><a class="nav-link" href="/assets/pdf/jaehongkim_cv.pdf">cv </a></li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jaehong</span> Kim </h1> <p class="desc">Jae Kim, 김재홍</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jaehong-profile-20240312-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jaehong-profile-20240312-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jaehong-profile-20240312-1400.webp"></source> <img src="/assets/img/jaehong-profile-20240312.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="jaehong-profile-20240312.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p><a href="mailto:%6A%61%65%68%6F%6E%67%39%35%30%33%30%35@%67%6D%61%69%6C.%63%6F%6D" title="email">jaehong.kim@inha.ac.kr <i class="fas fa-envelope"></i></a> <span>|</span> <a href="https://scholar.google.com/citations?user=U55MerIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer">Google Scholar <i class="ai ai-google-scholar"></i></a> <span>|</span> <a href="https://www.linkedin.com/in/jaykim305" title="LinkedIn" target="_blank" rel="noopener noreferrer">LinkedIn <i class="fab fa-linkedin"></i></a> <span>|</span> <a href="/assets/pdf/jaehongkim_cv.pdf" title="CV">CV <i class="fas fa-file-pdf "></i></a> <span>|</span> <a href="https://www.youtube.com/@jaehongkim9579" title="YouTube" target="_blank" rel="noopener noreferrer"> YouTube <i class="fab fa-youtube"></i> </a></p> <p><span title="Office Location"> <i class="fas fa-map-marker-alt"></i> Room 209, Building #5 (South) @ Inha University (5S209) </span></p> <p>Hi, I am an Assistant Professor in the Department of Artificial Intelligence at <a href="https://eng.inha.ac.kr/eng/3915/subview..do" target="_blank" rel="noopener noreferrer">Inha University</a>, where I lead the <strong>NAIS Lab</strong> (Networking, AI and Systems Lab). Prior to joining Inha, I was a postdoctoral researcher at Carnegie Mellon University, where I worked with <a href="https://www.cs.cmu.edu/~srini/" target="_blank" rel="noopener noreferrer">Prof. Srinivasan Seshan</a> and <a href="https://users.ece.cmu.edu/~agr/" target="_blank" rel="noopener noreferrer">Prof. Anthony Rowe</a>. Before that, I received my Ph.D. from KAIST, advised by <a href="https://ina.kaist.ac.kr/team/dongsuh" target="_blank" rel="noopener noreferrer">Prof. Dongsu Han</a>.</p> <p>My research interests lie in networked computer systems and applications, including multimedia applications, Internet services, cloud computing, and systems for AI. </p> <p><strong><i class="fas fa-lightbulb" style="color: #f0b429;"></i> I am actively looking for collaborations and recruiting graduate students (MS/Ph.D.). Undergrad students are also welcome. If you are highly motivated and interested in networking, AI-based multimedia, or systems for AI, please contact me at <a href="mailto:jaehong.kim@inha.ac.kr">jaehong.kim@inha.ac.kr</a></strong>.</p> <style>.education li{margin-top:5px}</style> <h2 style="margin-top:20px">Work Experience</h2> <div> <ul class="work experience"> <li> <b>Assistant Professor, Dept. of Artificial Intelligence, Inha University</b> <ul> <li>Sep. 2025 - Present</li> <li>Joint appointment in Dept. of Electrical and Computer Engineering</li> </ul> </li> <li> <b>Postdoctoral Research Associate, Computer Science Dept., Carnegie Mellon University</b> <ul> <li>Aug. 2024 - Aug. 2025</li> </ul> </li> </ul> </div> <h2 style="margin-top:20px">Educations</h2> <div> <ul class="education"> <li> <b>Ph.D., School of Electrical Engineering, KAIST</b> <ul> <li>Feb. 2020 - Aug. 2024, Advisor: Dongsu Han</li> </ul> </li> <li> <b>M.S., School of Electrical Engineering, KAIST</b> <ul> <li>Aug. 2018 - Feb. 2020, Advisor: Dongsu Han</li> </ul> </li> <li> <b>B.S., School of Electrical Engineering, KAIST</b> <ul> <li>March 2014 - Aug. 2018, Cum Laude</li> </ul> </li> </ul> </div> </div> <div class="publications"> <h2>Publications <span style="font-size: 0.7em;">(*:Co-first, ^: Co-corresponding)</span> </h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CVPR</abbr></div> <div id="nvp" class="col-sm-8"> <div class="title">Neural-Centric Video Processing Pipeline for Unified Multi-Task Inference (To appear)</div> <div class="author"> Seyeon Lee, Juncheol Ye, <em>Jaehong Kim</em>, and Dongsu Han</div> <div class="periodical"> <em>In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2026 </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">PAPER</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE VR</abbr></div> <div id="scenehub" class="col-sm-8"> <div class="title">SceneHub4D: A Dataset and Evaluation Framework for 6-DoF 4D VR Scenes</div> <div class="author"> <em>Jaehong Kim</em>, Tao Jin, Mallesham Dasari, Srinivasan Seshan, and Anthony Rowe</div> <div class="periodical"> <em>In IEEE Transactions on Visualization and Computer Graphics</em> 2026 <span class="bib-highlight">(Top submission, accepted to IEEE TVCG)</span> </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">PAPER</a> <a href="https://scenehub4d.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">WACV</abbr></div> <div id="nervast" class="col-sm-8"> <div class="title">NerVast: Compression-Efficient Scaling of Implicit Neural Video Representations via Scene-based Parameter-sharing</div> <div class="author"> Yunheon Lee, Juncheol Ye, <em>Jaehong Kim^</em>, and Dongsu Han^</div> <div class="periodical"> <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em> 2026 </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">PAPER</a> <a href="/assets/pdf/nervast-wacv26.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ISMAR</abbr></div> <div id="kim2025reconstruct" class="col-sm-8"> <div class="title">(Poster) Reconstructing Reality over Time: From Drone Capture to Timelapse Gaussian Splatting</div> <div class="author"> <em>Jaehong Kim</em>, Srinivasan Seshan, and Anthony Rowe</div> <div class="periodical"> <em>In IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)</em> 2025 </div> <div class="links"> <a href="https://ieeexplore.ieee.org/document/11236292" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> <a href="https://youtu.be/ZfpkKSmcY_4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DEMO</a> <a href="https://youtu.be/2vpA6KhxtjA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Fast Forward VIDEO</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NetAISys</abbr></div> <div id="lee2025presto" class="col-sm-8"> <div class="title">Presto: Hybrid CPU-GPU Preprocessing Framework for Video-based AI Inference System</div> <div class="author"> Jihyuk Lee, Dongsu Han, and <em>Jaehong Kim</em> </div> <div class="periodical"> <em>In Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3711875.3736688" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> </div> <div class="abstract hidden"> <p>The growing adoption of video-based AI models has created a pressing demand for high throughput, low latency inference systems. However, existing preprocessing frameworks—whether CPU or GPU based—struggle to keep up with the computational burdens of video decoding and data augmentation, resulting in suboptimal GPU utilization and degraded inference system performance.In this paper, we present PRESTO, a high-performance hybrid CPU-GPU preprocessing framework tailored for video-based AI inference systems. Presto integrates a hybrid preprocessing scheduler to dynamically balance CPU and GPU workloads, leverages selective decoding to eliminate unnecessary frame processing, and introduces a custom GPU Memory Manager that enables pipelined preprocessing and efficient GPU memory reuse. Through evaluation on the video captioning task, we show that Presto achieves up to 4.37\texttimes higher throughput and 2.72\texttimes lower latency compared to the de facto baselines, while reducing cloud costs by up to 75%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE Micro</abbr></div> <div id="lim2025efficient" class="col-sm-8"> <div class="title">Efficient Disaggregated Cloud Storage for Cold Videos with Neural Enhancement</div> <div class="author"> Jinyeong Lim, Juncheol Ye, <em>Jaehong Kim</em>, Hwijoon Lim, Hyunho Yeo, Junhyeok Jang, Myoungsoo Jung, and Dongsu Han</div> <div class="periodical"> <em>IEEE Micro</em> 2025 </div> <div class="links"> <a href="https://doi.ieeecomputersociety.org/10.1109/MM.2025.3562625" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">HotStorage</abbr></div> <div id="lim2023neural" class="col-sm-8"> <div class="title">Neural Cloud Storage: Innovative Cloud Storage Solution for Cold Video</div> <div class="author"> Jinyeong Lim, Juncheol Ye, <em>Jaehong Kim</em>, Hwijoon Lim, Hyunho Yeo, and Dongsu Han</div> <div class="periodical"> <em>In Proceedings of the 15th ACM Workshop on Hot Topics in Storage and File Systems</em> Jul 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3599691.3603401" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> </div> <div class="abstract hidden"> <p>Cloud storage providers offer different pricing tiers based on the access frequency of stored data. This pricing plan offers cost benefits for videos that are accessed less than once per month. However, the stringent requirement falls short in addressing the large number of "cold" videos stored today. This paper proposes Neural Cloud Storage (NCS), a pioneering approach to address the problem by applying neural enhancement, specifically content-aware super-resolution (SR). According to our preliminary cost-benefit analysis, NCS can further save an annual 14% total cost of ownership (TCO) compared to the cheapest AWS storage service for cold video. By reducing the cost, it expands the cold video coverage (from 25% to 38%) that can benefit from the multi-tiered service. As deep learning and computational resources continue to advance, we believe that neural enhancement will revolutionize the field of cloud storage.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EuroSys</abbr></div> <div id="lim2023flexpass" class="col-sm-8"> <div class="title">FlexPass: A Case for Flexible Credit-Based Transport for Datacenter Networks</div> <div class="author"> Hwijoon Lim, <em>Jaehong Kim</em>, Inho Cho, Keon Jang, Wei Bai, and Dongsu Han</div> <div class="periodical"> <em>In Proceedings of the Eighteenth European Conference on Computer Systems</em> Jul 2023 <span class="bib-highlight">(28th Samsung HumanTech Paper Award)</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3552326.3587453" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> <a href="https://ina.kaist.ac.kr/projects/flexpass/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Proactive transports explicitly allocate bandwidth to each sender with credits which schedule packet transmission. While promising, existing proactive solutions share a stringent deployment requirement; they assume the perfect control of every link and packet in the network. However, the assumption breaks in practice because new transports are usually deployed gradually over time and legacy traffic always coexists. In this paper, we present FlexPass, a credit-based transport that takes deployment flexibility as a first-class citizen. FlexPass uses a novel combination of network and end-host designs to solve the problem of co-existence and gradual deployment. FlexPass leverages a proactive control loop to send credit-scheduled packets and a complementary reactive control loop to send unscheduled packets to utilize the spare bandwidth. Finally, FlexPass prevents queue buildups of both scheduled and unscheduled packets, and recovers lost packets efficiently. Our evaluation on the testbed shows that FlexPass maintains co-existence with legacy transports (DCTCP), while preserving the high-performance properties of the proactive transport. In large-scale simulations, we show that FlexPass delivers the best incremental benefits during the gradual deployment. We find traffic upgraded to FlexPass benefits from the bounded queue and reduced flow completion time by up to 44% compared to the legacy traffic, while minimizing the side-effect on the legacy flows.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CoNEXT</abbr></div> <div id="kim2022cooptimizing" class="col-sm-8"> <div class="title">OutRAN: Co-Optimizing for Flow Completion Time in Radio Access Network</div> <div class="author"> <em>Jaehong Kim</em>, Yunheon Lee, Hwijoon Lim, Youngmok Jung, Song Min Kim, and Dongsu Han</div> <div class="periodical"> <em>In Proceedings of the 18th International Conference on Emerging Networking EXperiments and Technologies</em> Jul 2022 <span class="bib-highlight">(Best Paper Finalist, 29th Samsung HumanTech Paper Award)</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3555050.3569122" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> <a href="/assets/pdf/outran-conext22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://ina.kaist.ac.kr/projects/outran/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Traffic from interactive applications demanding low latency has become dominant in cellular networks. However, existing schedulers of cellular network base stations fall short in delivering low latency when prior information (i.e., dedicated Quality of Service (QoS)) is unavailable; they become service agnostic and perform towards maximizing the radio resource utilization or user fairness. We identify a new opportunity of providing a better latency for those latency-sensitive traffic flows by additionally taking the Flow Completion Time (FCT) into account in downlink scheduling at the base stations. However, the key challenges are 1) it can bring a severe cost in optimization metrics of the existing scheduler and 2) it should work without prior knowledge of the traffic.To this end, we present OutRAN, a practical flow scheduler designed for Radio Access Network that co-optimizes the FCT and optimization objectives of the cellular scheduler. The resulting system does not require prior information. Through simulation and over-the-air evaluation, we demonstrate that OutRAN outperforms the legacy LTE/5G schedulers in FCT, which leads to the reduction in webpage load time of Android phones.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SIGCOMM</abbr></div> <div id="yeo2022sigcomm" class="col-sm-8"> <div class="title">NeuroScaler: Neural Video Enhancement at Scale</div> <div class="author"> Hyunho Yeo, Hwijoon Lim, <em>Jaehong Kim</em>, Youngmok Jung, Juncheol Ye, and Dongsu Han</div> <div class="periodical"> <em>In Proceedings of the ACM SIGCOMM 2022 Conference</em> Jul 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544216.3544218" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> <a href="/assets/pdf/neuroscaler-sigcomm22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://ina.kaist.ac.kr/projects/neuroscaler/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>High-definition live streaming has experienced tremendous growth. However, the video quality of live video is often limited by the streamer’s uplink bandwidth. Recently, neural-enhanced live streaming has shown great promise in enhancing the video quality by running neural super-resolution at the ingest server. Despite its benefit, it is too expensive to be deployed at scale. To overcome the limitation, we present NeuroScaler, a framework that delivers efficient and scalable neural enhancement for live streams. First, to accelerate end-to-end neural enhancement, we propose novel algorithms that significantly reduce the overhead of video super-resolution, encoding, and GPU context switching. Second, to maximize the overall quality gain, we devise a resource scheduler that considers the unique characteristics of the neural-enhancing workload. Our evaluation on a public cloud shows NeuroScaler reduces the overall cost by 22.3\texttimes and 3.0–11.1\texttimes compared to the latest per-frame and selective neural-enhancing systems, respectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SIGCOMM</abbr></div> <div id="kim2020sigcomm" class="col-sm-8"> <div class="title">Neural-Enhanced Live Streaming: Improving Live Video Ingest via Online Learning</div> <div class="author"> <em>Jaehong Kim*</em>, Youngmok Jung*, Hyunho Yeo, Juncheol Ye, and Dongsu Han</div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication</em> Jul 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3387514.3405856" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> <a href="/assets/pdf/livenas-sigcomm20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://ina.kaist.ac.kr/projects/livenas/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Live video accounts for a significant volume of today’s Internet video. Despite a large number of efforts to enhance user quality of experience (QoE) both at the ingest and distribution side of live video, the fundamental limitations are that streamer’s upstream bandwidth and computational capacity limit the quality of experience of thousands of viewers.To overcome this limitation, we design LiveNAS, a new live video ingest framework that enhances the origin stream’s quality by leveraging computation at ingest servers. Our ingest server applies neural super-resolution on the original stream, while imposing minimal overhead on ingest clients. LiveNAS employs online learning to maximize the quality gain and dynamically adjusts the resource use to the real-time quality improvement. LiveNAS delivers high-quality live streams up to 4K resolution, outperforming WebRTC by 1.96 dB on average in Peak-Signal-to-Noise-Ratio on real video streams and network traces, which leads to 12%-69% QoE improvement for live stream viewers.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">OSDI</abbr></div> <div id="yeo2018osdi" class="col-sm-8"> <div class="title">Neural Adaptive Content-Aware Internet Video Delivery</div> <div class="author"> Hyunho Yeo, Youngmok Jung, <em>Jaehong Kim</em>, Jinwoo Shin, and Dongsu Han</div> <div class="periodical"> <em>In Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation</em> Jul 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.usenix.org/conference/osdi18/presentation/yeo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PAPER</a> <a href="/assets/pdf/osdi18-yeo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://ina.kaist.ac.kr/projects/nas/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Internet video streaming has experienced tremendous growth over the last few decades. However, the quality of existing video delivery critically depends on the bandwidth resource. Consequently, user quality of experience (QoE) suffers inevitably when network conditions become unfavorable. We present a new video delivery framework that utilizes client computation and recent advances in deep neural networks (DNNs) to reduce the dependency for delivering high-quality video. The use of DNNs enables us to enhance the video quality independent to the available bandwidth. We design a practical system that addresses several challenges, such as client heterogeneity, interaction with bitrate adaptation, and DNN transfer, in enabling the idea. Our evaluation using 3G and broadband network traces shows the proposed system outperforms the current state of the art, enhancing the average QoE by 43.08% using the same bandwidth budget or saving 17.13% of bandwidth while providing the same user QoE.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Jaehong Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: February 27, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-QD15PXCLPG"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QD15PXCLPG");</script> </body> </html>